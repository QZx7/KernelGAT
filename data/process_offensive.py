import json
import random
import requests
import spacy
from typing import Dict, List, Optional, Text, Tuple
from transformers import PegasusTokenizer, PegasusForConditionalGeneration
from transformers import AutoModelForSeq2SeqLM, AutoTokenizer
from transformers import AutoModelForCausalLM, pipeline

from tqdm import tqdm

# load spacy model
nlp = spacy.load("en_core_web_md")

def get_claim_evidence(original_data: Dict) -> Tuple[Dict, Dict]:
    id = 0
    # collect the claims and evidences as different sets
    claims = {}
    evidences = {}

    # get claims set and evidences set
    for example in original_data:
        claims[id] = example['explicit']
        evidences[id] = [[example['attribute'],
                         random.randrange(10),
                         example['implicit'],
                         random.random()]]
        for step in example['chain']:
            step_content = step.split('(')[0][:-1]
            evidences[id].append([example['attribute'],
                                 random.randrange(10),
                                 step_content,
                                 random.random()])
        id += 1

    return claims, evidences


def post_process_counter_step(origin_path: Text,
                              post_process_path: Text) -> None:

    original_file = open(origin_path, 'r', encoding='utf-8')
    post_file = open(post_process_path, 'w', encoding='utf-8')
    
    original_data = json.load(original_file)
    post_data = []

    for example in original_data:
        if example['chain'] == []:
            continue
        else:
            asr_only = True
            for step in example['chain']:
                if not '(ASR)' in step:
                    asr_only = False
                else:
                    step.replace(" (ASR)", "")
            if not asr_only:
                post_data.append(example)
    
    json.dump(post_data, post_file, ensure_ascii=False, indent=4)


def convert_to_fever_style(origin_path: Text,
                           fever_style_path: Text,
                           counter_example_path: Optional[Text]) -> None:
    """[summary]
    This is to covert the Mh-RIOT data to a FEVER style
    Args:
        origin_path (Text): path to the original Mh-RIOT data.
        fever_style_path (Text): path to the converted FEVER style data.
    """

    original_file = open(origin_path, 'r', encoding='utf-8')
    fever_style_file = open(fever_style_path, 'w', encoding='utf-8')
    
    original_data = json.load(original_file)
    
    tmp_data = {
        "id": 0,
        "claim": "",
        "evidence": [],
        "label": ""
    }
    # get claims set and evidences set
    claims, evidences = get_claim_evidence(original_data)
    counter_claims = None
    counter_evidences = None
    if counter_example_path:
        counter_example_file = open(counter_example_path, 'r', encoding='utf-8')
        counter_example_data = json.load(counter_example_file)
        counter_claims, counter_evidences = get_claim_evidence(counter_example_data)

    # generate test data
    # generate SUPPORTS examples
    for key, value in claims.items():
        tmp_data['id'] = key
        tmp_data['claim'] = claims[key]
        tmp_data['evidence'] = evidences[key]
        tmp_data['label'] = 'SUPPORTS'
        json.dump(tmp_data, fever_style_file)
        fever_style_file.write('\n')
    
    # generate REFUTES examples
    # no counterfactual examples provided, all examples are generated by exchanging evidences
    for key, value in counter_claims.items():
        tmp_data['id'] = len(claims) + key
        tmp_data['claim'] = value
        tmp_data['evidence'] = counter_evidences[key]
        tmp_data['label'] = 'REFUTES'
        json.dump(tmp_data, fever_style_file)
        fever_style_file.write('\n')
    for index in range(len(claims) - len(counter_claims)):
        tmp_data['id'] = len(claims) + len(counter_claims) + index
        random_claim = random.randrange(len(claims))
        tmp_data['claim'] = claims[random_claim]
        random_evidence = random.randrange(len(claims))
        while random_evidence == random_claim:
            random_evidence = random.randrange(len(claims))
        tmp_data['evidence'] = evidences[random_evidence]
        tmp_data['label'] = 'REFUTES'
        json.dump(tmp_data, fever_style_file)
        fever_style_file.write('\n')



    # for key, value in claims.items():
    #     tmp_data['id'] = len(claims) + key
    #     tmp_data['claim'] = claims[key]
    #     random_method = random.randrange(2)
    #     if not counter_evidences or (counter_evidences and random_method == 0):
    #         random_evidence = random.randrange(len(claims))
    #         while random_evidence == key:
    #             random_evidence = random.randrange(len(claims))
    #         tmp_data['evidence'] = evidences[random_evidence]
    #         tmp_data['label'] = 'REFUTES'
    #     elif counter_evidences and random_method == 1:
    #         tmp_data['evidence'] = counter_evidences[key]
    #         tmp_data['label'] = 'REFUTES'
    #     json.dump(tmp_data, fever_style_file)
    #     fever_style_file.write('\n')
    
    # generate NOT ENOUGH INFO examples
    for key, value in claims.items():
        tmp_data['id'] = 2 * len(claims) + key
        tmp_data['claim'] = claims[key]
        random_removed_evidence = random.randrange(len(evidences[key]))
        evidences[key].pop(random_removed_evidence)
        tmp_data['evidence'] = evidences[key]
        tmp_data['label'] = 'NOT ENOUGH INFO'
        json.dump(tmp_data, fever_style_file)
        fever_style_file.write('\n')

def remove_rephrasing_steps(origin_path: Text) -> None:
    """[summary]
    This is to remove the rephrasing steps in the original chain.
    It will create a new file with same file name but with a '_non_rr' suffix.
    Args:
        origin_path (Text): The path to the original file
    """
    original_file = open(origin_path, 'r', encoding='utf-8')
    new_file_path = origin_path[:-5] + '_non_rr.json'
    print(new_file_path)
    new_file = open(new_file_path, 'w', encoding='utf-8')

    original_data = json.load(original_file)
    for example in original_data:
        new_chain = []
        for step in example['chain']:
            step_tag = step.split('(')[1][:-1]
            if step_tag != 'RR':
                new_chain.append(step)
        example['chain'] = new_chain

    json.dump(original_data, new_file, indent=4)

class ParaphrasingModel:
    """The class to initiate a paraphrasing model from the given name
    """

    def __init__(self, name) -> None:
        self.name = name
        self.model = None
        self.tokenizer = None

    def load_paraphrasing_model(self) -> None:
        """[summary]
        This is to load the paraphrasing models.
        Args:
            name (Text): name of the paraphrasing models.
                        current avaliable names: pegasus, t5_paws
        Returns:
            Tuple[PreTrainedModel, PreTrainedTokenizer]: Return the loaded model and tokenizer
        """
        # for pegasus
        if self.name == "pegasus":
            model_name = 'tuner007/pegasus_paraphrase'
            # torch_device = 'cuda' if torch.cuda.is_available() else 'cpu'
            self.tokenizer = PegasusTokenizer.from_pretrained(model_name)
            self.model = PegasusForConditionalGeneration.from_pretrained(model_name)
        elif self.name == "t5_paws":
            model_name = 'Vamsi/T5_Paraphrase_Paws'
            self.tokenizer = AutoTokenizer.from_pretrained(model_name)
            self.model = AutoModelForSeq2SeqLM.from_pretrained(model_name)
        
    def get_paraphrase(self, input: Text) -> Text:
        """[summary]
        This is to get a paraphrase of the input text.
        Args:
            input (Text): the input text

        Returns:
            Text: the returned paraphrasing
        """
        tgt_text = ""
        self.load_paraphrasing_model()
        if self.name == "pegasus":
            batch = self.tokenizer([input],truncation=True,padding='longest',max_length=60, return_tensors="pt")
            translated = self.model.generate(**batch,max_length=60,num_beams=10, num_return_sequences=1, temperature=1.5)
            tgt_text = self.tokenizer.batch_decode(translated, skip_special_tokens=True)
        elif self.name == "t5_paws":
            source = "paraphrase: " + input + " </s>"
            encoding = self.tokenizer.encode_plus(source,pad_to_max_length=True, return_tensors="pt")
            input_ids, attention_masks = encoding["input_ids"].to("cuda"), encoding["attention_mask"].to("cuda")
            outputs = self.model.generate(
                input_ids=input_ids, attention_mask=attention_masks,
                max_length=256,
                do_sample=True,
                top_k=120,
                top_p=0.95,
                early_stopping=True,
                num_return_sequences=1
            )
            for output in outputs:
                tgt_text = self.tokenizer.decode(output, skip_special_tokens=True,clean_up_tokenization_spaces=True)
        
        return tgt_text

class CounterFactualModel:
    """[summary]
    This class generate general-purpose counterfactual sentences.
    """

    def __init__(self, name) -> None:
        self.name = name
        self.model = None
        self.tokenizer = None
        self.generator = None
    
    def load_model(self) -> None:
        """[summary]
        This is to load the counterfactual models.
        Args:
            name (Text): name of the counterfactual models.
                        current avaliable names: polyjuice
        Returns:
            Tuple[PreTrainedModel, PreTrainedTokenizer]: Return the loaded model and tokenizer
        """
        # for pegasus
        if self.name == "polyjuice":
            model_name = 'uw-hai/polyjuice'
            self.tokenizer = AutoTokenizer.from_pretrained(model_name)
            self.model = AutoModelForCausalLM.from_pretrained(model_name)
            self.generator = pipeline("text-generation",
                model=self.model,
                tokenizer=self.tokenizer,
                framework="pt"
            )
        
    def get_counterfactual(self, input: Text) -> Text:
        """[summary]
        This is to get a paraphrase of the input text.
        Args:
            input (Text): the input text

        Returns:
            Text: the returned paraphrasing
        """
        result_text = self.generator(input, num_beams=3, num_return_sequences=1)
        return result_text

def generate_counter_evidence(origin_path: Text, counter_evidence_path: Text, model: CounterFactualModel) -> None:
    """This is to generate some counterfactual examples
    using the counterfactual paraphrasing models.
    Args:
        origin_path (Text): path to the original file.
        counter_evidence_path (Text): path of the to be generated counterfactual file.
        model (CounterFactualModel): The counterfactual model being used.
    """
    model.load_model()
    original_file = open(origin_path, 'r', encoding='utf-8')
    counter_evidence_file = open(counter_evidence_path, 'w', encoding='utf-8')

    original_data = json.load(original_file)
    counter_data = []
    # for each example in Mh-RIOT
    for example in tqdm(original_data):
        counter_example = example.copy()
        # each step in the chain
        for index in range(len(counter_example['chain'])):
            step_tag = counter_example['chain'][index].split('(')[1][:-1]
            step_content = counter_example['chain'][index].split('(')[0][:-1]
            # only get counterfactual examples for KIR steps
            if step_tag == 'KIR':
                counter_input = step_content + "<|perturb|>[negation]"
                counter_samples = model.get_counterfactual(counter_input)
                # iterate all generated counterfactual examples
                for sample in counter_samples:
                    try:
                        answer = sample['generated_text'].split("[SEP]")[1][1:].split("[A")[0][:-1]
                        sentence = sample['generated_text'].split("[negation]")[1][1:].split("[SEP]")[0][:-1]
                        sentence = sentence.replace("[BLANK]", answer)
                        new_step = sentence + " (KIR)"
                        counter_example['chain'][index] = new_step
                        counter_data.append(counter_example)
                    except IndexError:
                        pass
    json.dump(original_data, counter_evidence_file, indent=4)


def generate_counter_nlp(origin_path: Text, counter_evidence_path:Text) -> None:
    """This is to process the splited KIR steps.

    Args:
        origin_path (Text): path to the original file.
        counter_evidence_path (Text): path of the to be generated counterfactual file.
    """
    original_file = open(origin_path, 'r', encoding='utf-8')
    counter_evidence_file = open(counter_evidence_path, 'w', encoding='utf-8')
    original_data = json.load(original_file)

    for example in tqdm(original_data):
        counter_chain = []
        for step in example["chain"]:
            step_parts = step.split("(")
            step_tag = step_parts[1][:-1]
            step_content = step_parts[0][:-1]
            if step_tag == "KIR":
                # process the KIR steps
                if "because" in step_content and "because of" not in step_content:
                    content_parts = step_content.split("because")
                    # print(content_parts)
                    try:
                        new_content_parts = counter_KIR_process(content_parts, paraphrasing=True)
                    except IndexError:
                        pass
                    # print(new_content_parts)
                    counter_chain.extend(new_content_parts)
            else:
                counter_chain.append(step)
        example['chain'] = counter_chain
    json.dump(original_data, counter_evidence_file, indent=4, ensure_ascii=False)


def counter_KIR_process(sentence_pieces: List, paraphrasing: Optional[bool] = False) -> List[str]:
    new_sentence_pieces = []
    if paraphrasing:
        counterfactual = CounterFactualModel("polyjuice")
        counterfactual.load_model()
    for sentence in sentence_pieces:
        if paraphrasing:
            promt = sentence + " <|perturb|>[negation]"
            counter_samples = counterfactual.get_counterfactual(promt)
            for sample in counter_samples:
                answer = sample['generated_text'].split("[SEP]")[1][1:].split("[A")[0][:-1]
                sentence = sample['generated_text'].split("[negation]")[1][1:].split("[SEP]")[0][:-1]
                sentence = sentence.replace("[BLANK]", answer)
        else:
            # remove don't, can't
            # haven't to have, have to haven't
            # cannot to can
            # should to "don't have to"
            # "have to", "need to" to "don't have to"
            if "don\'t" in sentence or "can\'t" in sentence:
                sentence = sentence.replace(" don\'t ", " ").replace(" can\'t ", " ")
            elif "cannot" in sentence:
                sentence = sentence.replace(" cannot ", " can ")
            elif "should" in sentence:
                sentence = sentence.replace(" should ", " don\'t have to ")
            elif "have to" in sentence:
                sentence = sentence.replace(" have to ", " don\'t have to ")
        new_sentence_pieces.append(sentence)
    return new_sentence_pieces
            # doc = nlp(sentence)
            # for noun_chunk in doc.noun_chunks:
            #     if noun_chunk.text not in ["you", "You", "yourself", "Yourself"]:
            #         print(noun_chunk.text)
            # print("\n")


def concept_net_request(message: Text) -> List[str]:
    message = message.replace(" ", "_")
    search_url = "http://api.conceptnet.io/c/en/" + message
    obj = requests.get(search_url).json()
    for edge in obj["edges"]:

        print(edge["start"]["label"] + "---" + edge["rel"]["label"] + "---" + edge["end"]["label"])
        if 'surfaceText' in edge:
            print(edge['surfaceText'])
        # print(edge["rel"]["label"])
        # print(edge["end"]["label"] + "\n")

if __name__ == '__main__':
    # convert_to_fever_style(r'./offensive_data/origin/non_offensive_grammar.json',
    #                        r'./offensive_data/FEVER_Style/bert_test.jsonl')
    # remove_rephrasing_steps(r'./offensive_data/origin/non_offensive_grammar.json')
    convert_to_fever_style(r'./data/offensive_data/origin/non_offensive_grammar_non_rr.json',
                           r'./data/offensive_data/FEVER_Style/bert_test_non_rr_counter.jsonl',
                           counter_example_path=r'./data/offensive_data/origin/non_offensive_non_rr_counter_filtered - Copy.json')
    # generate_counter_evidence(r'./', r'./')
    # counter_factual = CounterFactualModel("polyjuice")
    # counter_factual.load_model()
    # counter_samples = counter_factual.get_counterfactual("I think you are not able to sing in the church choir if there were an audition. <|perturb|>[negation]")
    # print(counter_samples)
    # for sample in counter_samples:
    #     answer = sample['generated_text'].split("[SEP]")[1][1:].split("[A")[0][:-1]
    #     sentence = sample['generated_text'].split("[negation]")[1][1:].split("[SEP]")[0][:-1]
    #     sentence = sentence.replace("[BLANK]", answer)
    #     print(sentence)
    # generate_counter_evidence(r'./offensive_data/origin/non_offensive_grammar_non_rr.json',
    #                           r'./offensive_data/origin/non_offensive_grammar_counter.json',
    #                           counter_factual)

    # generate_counter_nlp(r'./data/offensive_data/origin/non_offensive_grammar_non_rr.json',
    #                      r'./data/offensive_data/origin/non_offensive_non_rr_counter.json')
    # post_process_counter_step(r'./data/offensive_data/origin/non_offensive_non_rr_counter.json',
    #                           r'./data/offensive_data/origin/non_offensive_non_rr_counter_filtered.json')
    # concept_net_request("reputation")
